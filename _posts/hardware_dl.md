---
layout: default
---

## Backgrounds
As the repaid growth of the data scale and the complexity of the deep learning network structure, the demand for the model training turns to be higher and higher. Different from the traditional machine learning algorithms, deep learning tends to be trained with GPU rather than CPU, and the GPU can accelerate the training process in hundreds even thousands times when comparing with CPU. In order to advancing accelerate the DL model training, the computational architecture of Multi-GPUs or GPU-Grid has been prevalently used in the passed several years.  There always no limitations for pursuing outstanding, Tensor Processing Unit (TPU), a modern customised ASICS 


_yay_

[back](./)
