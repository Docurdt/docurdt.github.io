---
layout: post
author: Docurdt
---

## Backgrounds
As the scale of data size and the complexity of the deep learning network architectures continues to grow, the hardware requirements for training deep learning models are becoming higher and higher. Unlike traditional machine learning algorithms, deep learning tends to use GPUs instead of CPUs for model training. Compared to CPUs, GPUs can speed up the training process by hundreds or even thousands of times. To further accelerate DL model training, multi-GPU or GPU-Grid computing architectures have become commonplace in the past few years..  There always no limitations for pursuing outstanding, Tensor Processing Unit (TPU), a modern customised ASICS


_yay_

[back](./)
