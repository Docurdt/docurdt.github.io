---
layout: post
author: Docurdt
date: 2018-11-04
---

## Backgrounds
As the scale of data size and the complexity of the deep learning network architectures continues to grow, the hardware requirements for training deep learning models are becoming higher and higher. Unlike traditional machine learning algorithms, deep learning tends to use GPUs instead of CPUs for model training. Compared to CPUs, GPUs can speed up the training process by hundreds or even thousands of times. To further accelerate DL model training, multi-GPU or GPU-Grid computing architectures have become commonplace in the past few years. There is always no limit to the pursuit of faster and more powerful performance. Tensor Processing Unit (TPU) was announced by Google in 2016 at Google/IO, which is a customised ASICS that has been specifically designed and optimised for Google's Tensorflow framework. In which, Tensorflow is an open-sourced symbolic math library for deep learning, including many kinds of neural networks and recurrent neural networks, and was proposed by Google in November of 2015. And since then, TPU was used inside Google in the Data Center.


[back](../../../blog.html)
